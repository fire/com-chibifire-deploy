# This is based on the cockroachdb example
apiVersion: v1
kind: Service
metadata:
  # This service is meant to be used by clients of the database. It exposes a ClusterIP that will
  # automatically load balance connections to the different database pods.
  name: snappydata-public
  labels:
    app: snappydata
spec:
  ports:
  - port: 1527
    targetPort: 1527
    name: jdbc
  - port: 1531
    targetPort: 1531
    name: thrift
  - port: 4040
    targetPort: 4040
    name: spark-ui 
  - port: 10334
    targetPort: 10334
    name: locator
  type: LoadBalancer
  selector:
    app: snappydata
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: snappydata-locator
  labels:
    app: snappydata-locator
  annotations:
    # This is needed to make the peer-finder work properly and to help avoid
    # edge cases where instance 0 comes up after losing its data and needs to
    # decide whether it should create a new cluster or try to join an existing
    # one. If it creates a new cluster when it should have joined an existing
    # one, we'd end up with two separate clusters listening at the same service
    # endpoint, which would be very bad.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: "_status/vars"
    prometheus.io/port: "8080"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  - port: 10334
    targetPort: 10334
    name: locator
  - port: 3768
    targetPort: 3768
    name: zeppelin
  - port: 1531
    targetPort: 1531
    name: thrift
  - port: 1527
    targetPort: 1527
    name: jdbc
  clusterIP: None
  selector:
    app: snappydata-locator
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: snappydata-server
  labels:
    app: snappydata-server
  annotations:
    # This is needed to make the peer-finder work properly and to help avoid
    # edge cases where instance 0 comes up after losing its data and needs to
    # decide whether it should create a new cluster or try to join an existing
    # one. If it creates a new cluster when it should have joined an existing
    # one, we'd end up with two separate clusters listening at the same service
    # endpoint, which would be very bad.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: "_status/vars"
    prometheus.io/port: "8080"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  - port: 10334
    targetPort: 10334
    name: locator
  - port: 3768
    targetPort: 3768
    name: zeppelin
  - port: 1531
    targetPort: 1531
    name: thrift
  - port: 1527
    targetPort: 1527
    name: jdbc
  clusterIP: None
  selector:
    app: snappydata-server
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: snappydata-leader
  labels:
    app: snappydata-leader
  annotations:
    # This is needed to make the peer-finder work properly and to help avoid
    # edge cases where instance 0 comes up after losing its data and needs to
    # decide whether it should create a new cluster or try to join an existing
    # one. If it creates a new cluster when it should have joined an existing
    # one, we'd end up with two separate clusters listening at the same service
    # endpoint, which would be very bad.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: "_status/vars"
    prometheus.io/port: "8080"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  - port: 10334
    targetPort: 10334
    name: locator
  - port: 3768
    targetPort: 3768
    name: zeppelin
  - port: 1531
    targetPort: 1531
    name: thrift
  - port: 1527
    targetPort: 1527
    name: jdbc
  clusterIP: None
  selector:
    app: snappydata-leader
---
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
  name: snappydata-locator
spec:
  serviceName: "snappydata-locator"
  replicas: 1
  template:
    metadata:
      labels:
        app: snappydata-locator
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      containers:
      - name: snappydata-locator
        # Runs the current snappydata release
        image: snappydatainc/snappydata
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        - containerPort: 10334
          name: locator
        - containerPort: 1527
          name: jdbc
        #livenessProbe:
        #  httpGet:
        #    path: /_admin/v1/health
        #    port: http
        #  initialDelaySeconds: 30
        #readinessProbe:
        #  httpGet:
        #    path: /_admin/v1/health
        #    port: http
        # initialDelaySeconds: 10
        volumeMounts:
        - name: datadir
          mountPath: /snappydata
        command:
          - "/bin/bash"
          - "-ecx"
          - |
            # The use of qualified `hostname -f` is crucial:
            # Other nodes aren't able to look up the unqualified hostname.
            SNARGS=()
            # We only want to initialize a new cluster (by omitting the join flag)
            # if we're sure that we're the first node (i.e. index 0) and that
            # there aren't any other nodes running as part of the cluster that
            # this is supposed to be a part of (which indicates that a cluster
            # already exists and we should make sure not to create a new one).
            # It's fine to run without --join on a restart if there aren't any
            # other nodes.
            ##if [ ! "$(hostname)" == "cockroachdb-0" ] || \
            ##   [ -e ##"/cockroach/cockroach-data/cluster_exists_marker" ]
            ##then
              # We don't join cockroachdb in order to avoid a node attempting
              # to join itself, which currently doesn't work
              # (https://github.com/cockroachdb/cockroach/issues/9625).
              ##CRARGS+=("--join" "cockroachdb-public")
            #fi
            #exec /cockroach/cockroach ${CRARGS[*]}
            exec /bin/bash -c "mkdir -p /snappydata/locator && /opt/snappydata/bin/snappy-shell locator start -dir=/snappydata/locator -peer-discovery-address=$(hostname -f) -client-bind-address=$(hostname -f) -J-Duser.timezone=UTC && tail -f /snappydata/locator/snappylocator.log"
      # No pre-stop hook is required, a SIGTERM plus some time is all that's
      # needed for graceful shutdown of a node.
      terminationGracePeriodSeconds: 60
      volumes:
      - name: datadir
        persistentVolumeClaim:
          claimName: datadir
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 5Gi
---
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
  name: snappydata-server
spec:
  serviceName: "snappydata-server"
  replicas: 1
  template:
    metadata:
      labels:
        app: snappydata-server
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      containers:
      - name: snappydata-server
        # Runs the current snappydata release
        image: snappydatainc/snappydata
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        - containerPort: 10334
          name: locator
        - containerPort: 1527
          name: jdbc
        #livenessProbe:
        #  httpGet:
        #    path: /_admin/v1/health
        #    port: http
        #  initialDelaySeconds: 30
        #readinessProbe:
        #  httpGet:
        #    path: /_admin/v1/health
        #    port: http
        # initialDelaySeconds: 10
        volumeMounts:
        - name: datadir
          mountPath: /snappydata
        command:
          - "/bin/bash"
          - "-ecx"
          - |
            # The use of qualified `hostname -f` is crucial:
            # Other nodes aren't able to look up the unqualified hostname.
            SNARGS=()
            # We only want to initialize a new cluster (by omitting the join flag)
            # if we're sure that we're the first node (i.e. index 0) and that
            # there aren't any other nodes running as part of the cluster that
            # this is supposed to be a part of (which indicates that a cluster
            # already exists and we should make sure not to create a new one).
            # It's fine to run without --join on a restart if there aren't any
            # other nodes.
            ##if [ ! "$(hostname)" == "cockroachdb-0" ] || \
            ##   [ -e ##"/cockroach/cockroach-data/cluster_exists_marker" ]
            ##then
              # We don't join cockroachdb in order to avoid a node attempting
              # to join itself, which currently doesn't work
              # (https://github.com/cockroachdb/cockroach/issues/9625).
              ##CRARGS+=("--join" "cockroachdb-public")
            #fi
            #exec /cockroach/cockroach ${CRARGS[*]}
            exec /bin/bash -c "mkdir -p /snappydata/server && /opt/snappydata/bin/snappy-shell server start -dir=/snappydata/server/ -bind-address=$(hostname -f) -locators=snappydata-locator:10334, -dir=/snappydata/server -client-bind-address=$(hostname -f) -J-Duser.timezone=UTC -J-Djava.net.preferIPv4Stack=true -thrift-server-address=$(hostname -f) -heap-size=4096m -thrift-server-port=1531 -thrift-binary-protocol=true && tail -f /snappydata/server/snappyserver.log"
      # No pre-stop hook is required, a SIGTERM plus some time is all that's
      # needed for graceful shutdown of a node.
      terminationGracePeriodSeconds: 60
      volumes:
      - name: datadir
        persistentVolumeClaim:
          claimName: datadir
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 5Gi
---
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
  name: snappydata-leader
spec:
  serviceName: "snappydata-leader"
  replicas: 1
  template:
    metadata:
      labels:
        app: snappydata-leader
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      containers:
      - name: snappydata-leader
        # Runs the current snappydata release
        image: snappydatainc/snappydata
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        - containerPort: 10334
          name: locator
        - containerPort: 1527
          name: jdbc
        #livenessProbe:
        #  httpGet:
        #    path: /_admin/v1/health
        #    port: http
        #  initialDelaySeconds: 30
        #readinessProbe:
        #  httpGet:
        #    path: /_admin/v1/health
        #    port: http
        # initialDelaySeconds: 10
        volumeMounts:
        - name: datadir
          mountPath: /snappydata
        command:
          - "/bin/bash"
          - "-ecx"
          - |
            # The use of qualified `hostname -f` is crucial:
            # Other nodes aren't able to look up the unqualified hostname.
            SNARGS=()
            # We only want to initialize a new cluster (by omitting the join flag)
            # if we're sure that we're the first node (i.e. index 0) and that
            # there aren't any other nodes running as part of the cluster that
            # this is supposed to be a part of (which indicates that a cluster
            # already exists and we should make sure not to create a new one).
            # It's fine to run without --join on a restart if there aren't any
            # other nodes.
            ##if [ ! "$(hostname)" == "cockroachdb-0" ] || \
            ##   [ -e ##"/cockroach/cockroach-data/cluster_exists_marker" ]
            ##then
              # We don't join cockroachdb in order to avoid a node attempting
              # to join itself, which currently doesn't work
              # (https://github.com/cockroachdb/cockroach/issues/9625).
              ##CRARGS+=("--join" "cockroachdb-public")
            #fi
            #exec /cockroach/cockroach ${CRARGS[*]}
            exec /bin/bash -c "mkdir -p /snappydata/leader && /opt/snappydata/bin/snappy-shell leader start -dir=/snappydata/leader -locators=snappydata-locator:10334 -client-bind-address=$(hostname -f) -J-Duser.timezone=UTC && tail -f /snappydata/leader/snappyleader.log"
      # No pre-stop hook is required, a SIGTERM plus some time is all that's
      # needed for graceful shutdown of a node.
      terminationGracePeriodSeconds: 60
      volumes:
      - name: datadir
        persistentVolumeClaim:
          claimName: datadir
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 5Gi