# This is based on the cockroachdb example
apiVersion: v1
kind: Service
metadata:
  # This service is meant to be used by clients of the database. It exposes a ClusterIP that will
  # automatically load balance connections to the different database pods.
  name: snappydata-public
  labels:
    app: snappydata
spec:
  ports:
  - port: 1527
    targetPort: 1527
    name: jdbc
  - port: 1531
    targetPort: 1531
    name: thrift
  type: LoadBalancer
  selector:
    app: snappydata-server
---
# This is based on the cockroachdb example
apiVersion: v1
kind: Service
metadata:
  # This service is meant to be used by clients of the database. It exposes a ClusterIP that will
  # automatically load balance connections to the different database pods.
  name: snappydata-leader-public
  labels:
    app: snappydata
spec:
  ports:
  - port: 4040
    targetPort: 4040
    name: spark-ui
  type: LoadBalancer
  selector:
    app: snappydata-leader
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: snappydata-locator
  labels:
    app: snappydata-locator
  annotations:
    # This is needed to make the peer-finder work properly and to help avoid
    # edge cases where instance 0 comes up after losing its data and needs to
    # decide whether it should create a new cluster or try to join an existing
    # one. If it creates a new cluster when it should have joined an existing
    # one, we'd end up with two separate clusters listening at the same service
    # endpoint, which would be very bad.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: "_status/vars"
    prometheus.io/port: "8080"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  - port: 10334
    targetPort: 10334
    name: locator
  - port: 3768
    targetPort: 3768
    name: zeppelin
  - port: 1531
    targetPort: 1531
    name: thrift
  - port: 1527
    targetPort: 1527
    name: jdbc
  clusterIP: None
  selector:
    app: snappydata-locator
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: snappydata-server
  labels:
    app: snappydata-server
  annotations:
    # This is needed to make the peer-finder work properly and to help avoid
    # edge cases where instance 0 comes up after losing its data and needs to
    # decide whether it should create a new cluster or try to join an existing
    # one. If it creates a new cluster when it should have joined an existing
    # one, we'd end up with two separate clusters listening at the same service
    # endpoint, which would be very bad.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: "_status/vars"
    prometheus.io/port: "8080"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  - port: 10334
    targetPort: 10334
    name: locator
  - port: 3768
    targetPort: 3768
    name: zeppelin
  - port: 1531
    targetPort: 1531
    name: thrift
  - port: 1527
    targetPort: 1527
    name: jdbc
  clusterIP: None
  selector:
    app: snappydata-server
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: snappydata-leader
  labels:
    app: snappydata-leader
  annotations:
    # This is needed to make the peer-finder work properly and to help avoid
    # edge cases where instance 0 comes up after losing its data and needs to
    # decide whether it should create a new cluster or try to join an existing
    # one. If it creates a new cluster when it should have joined an existing
    # one, we'd end up with two separate clusters listening at the same service
    # endpoint, which would be very bad.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: "_status/vars"
    prometheus.io/port: "8080"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  - port: 10334
    targetPort: 10334
    name: locator
  - port: 3768
    targetPort: 3768
    name: zeppelin
  - port: 1531
    targetPort: 1531
    name: thrift
  - port: 1527
    targetPort: 1527
    name: jdbc
  clusterIP: None
  selector:
    app: snappydata-leader
---
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
  name: snappydata-locator
spec:
  serviceName: "snappydata-locator"
  replicas: 2
  template:
    metadata:
      labels:
        app: snappydata-locator
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      containers:
      - name: snappydata-locator
        # Runs the current snappydata release
        image: snappydatainc/snappydata
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        - containerPort: 10334
          name: locator
        - containerPort: 1527
          name: jdbc
        livenessProbe:
          tcpSocket:
            port: 10334
          initialDelaySeconds: 30
        readinessProbe:
          tcpSocket:
            port: 10334
          initialDelaySeconds: 80
        volumeMounts:
        - name: datadir
          mountPath: /snappydata
        command:
          - "/bin/bash"
          - "-ecx"
          - |
            exec /bin/bash -c "mkdir -p /snappydata/locator && /opt/snappydata/bin/snappy-shell locator start -dir=/snappydata/locator -peer-discovery-address=$(hostname -f) -client-bind-address=$(hostname -f) -J-Duser.timezone=UTC && tail -f /snappydata/locator/snappylocator.log"
        lifecycle:
          preStop:
            exec:
              command:
              - /opt/snappydata/bin/snappy-shell locator stop -dir=/snappydata/locator
      terminationGracePeriodSeconds: 60
      volumes:
      - name: datadir
        persistentVolumeClaim:
          claimName: datadir
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 5Gi
---
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
  name: snappydata-server
spec:
  serviceName: "snappydata-server"
  replicas: 1
  template:
    metadata:
      labels:
        app: snappydata-server
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      containers:
      - name: snappydata-server
        # Runs the current snappydata release
        image: snappydatainc/snappydata
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        - containerPort: 10334
          name: locator
        - containerPort: 1527
          name: jdbc
        livenessProbe:
          tcpSocket:
            port: 1527
          initialDelaySeconds: 30
        readinessProbe:
          tcpSocket:
            port: 1527
          initialDelaySeconds: 80
        volumeMounts:
        - name: datadir
          mountPath: /snappydata
        command:
          - "/bin/bash"
          - "-ecx"
          - |
            exec /bin/bash -c "mkdir -p /snappydata/server && /opt/snappydata/bin/snappy-shell server start -dir=/snappydata/server -bind-address=$(hostname -f) -locators=snappydata-locator:10334, -dir=/snappydata/server -client-bind-address=$(hostname -f) -J-Duser.timezone=UTC -J-Djava.net.preferIPv4Stack=true -thrift-server-address=$(hostname -f) -heap-size=4096m -thrift-server-port=1531 -thrift-binary-protocol=true && tail -f /snappydata/server/snappyserver.log"
        lifecycle:
          preStop:
            exec:
              command:
              - /opt/snappydata/bin/snappy-shell server stop -dir=/snappydata/server
      terminationGracePeriodSeconds: 60
      volumes:
      - name: datadir
        persistentVolumeClaim:
          claimName: datadir
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 5Gi
---
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
  name: snappydata-leader
spec:
  serviceName: "snappydata-leader"
  replicas: 1
  template:
    metadata:
      labels:
        app: snappydata-leader
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      containers:
      - name: snappydata-leader
        # Runs the current snappydata release
        image: snappydatainc/snappydata
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        - containerPort: 10334
          name: locator
        - containerPort: 1527
          name: jdbc
        livenessProbe:
          httpGet:
            path: /
            port: 4040
          initialDelaySeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 4040
          initialDelaySeconds: 80
        volumeMounts:
        - name: datadir
          mountPath: /snappydata
        command:
          - "/bin/bash"
          - "-ecx"
          - |
            exec /bin/bash -c "mkdir -p /snappydata/leader && /opt/snappydata/bin/snappy-shell leader start -dir=/snappydata/leader -locators=snappydata-locator:10334 -client-bind-address=$(hostname -f) -J-Duser.timezone=UTC && tail -f /snappydata/leader/snappyleader.log"
        lifecycle:  
          preStop: 
            exec:
              command:
              - /opt/snappydata/bin/snappy-shell leader stop -dir=/snappydata/leader
      terminationGracePeriodSeconds: 60
      volumes:
      - name: datadir
        persistentVolumeClaim:
          claimName: datadir
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 5Gi