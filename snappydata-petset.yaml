# This is based on the cockroachdb example
apiVersion: v1
kind: Service
metadata:
  # This service is meant to be used by clients of the database. It exposes a ClusterIP that will
  # automatically load balance connections to the different database pods.
  name: snappydata-public
  labels:
    app: snappydata
spec:
  ports:
  - port: 1527
    targetPort: 1527
    name: jdbc
  - port: 1531
    targetPort: 1531
    name: thrift
  type: LoadBalancer
  selector:
    app: snappydata-server
---
apiVersion: v1
kind: Service
metadata:
  # This service is meant to be used by clients of the database. It exposes the locator so that the
  # servers can find other servers.
  name: snappydata-locator-public
  labels:
    app: snappydata
spec:
  ports:
  - port: 1527
    targetPort: 1527
    name: jdbc
  - port: 10334
    targetPort: 10334
    name: locator
  selector:
    app: snappydata-locator
---
apiVersion: v1
kind: Service
metadata:
  # This service is meant to be used by clients of the database. It exposes a ClusterIP that will
  # automatically load balance connections to the different database pods.
  name: snappydata-server-public
  labels:
    app: snappydata
spec:
  ports:
  - port: 1527
    targetPort: 1527
    name: jdbc
  - port: 10334
    targetPort: 10334
    name: locator
  selector:
    app: snappydata-server
---
apiVersion: v1
kind: Service
metadata:
  # This service is meant to be used by clients of the database. It exposes a ClusterIP that will
  # automatically load balance connections to the different database pods.
  name: snappydata-leader-public
  labels:
    app: snappydata
spec:
  ports:
  - port: 4040
    targetPort: 4040
    name: spark-ui
  selector:
    app: snappydata-leader
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: snappydata-locator
  labels:
    app: snappydata-locator
  annotations:
    # This is needed to make the peer-finder work properly and to help avoid
    # edge cases where instance 0 comes up after losing its data and needs to
    # decide whether it should create a new cluster or try to join an existing
    # one. If it creates a new cluster when it should have joined an existing
    # one, we'd end up with two separate clusters listening at the same service
    # endpoint, which would be very bad.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: "_status/vars"
    prometheus.io/port: "8080"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  - port: 10334
    targetPort: 10334
    name: locator
  - port: 3768
    targetPort: 3768
    name: zeppelin
  - port: 1531
    targetPort: 1531
    name: thrift
  - port: 1527
    targetPort: 1527
    name: jdbc
  clusterIP: None
  selector:
    app: snappydata-locator
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: snappydata-server
  labels:
    app: snappydata-server
  annotations:
    # This is needed to make the peer-finder work properly and to help avoid
    # edge cases where instance 0 comes up after losing its data and needs to
    # decide whether it should create a new cluster or try to join an existing
    # one. If it creates a new cluster when it should have joined an existing
    # one, we'd end up with two separate clusters listening at the same service
    # endpoint, which would be very bad.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: "_status/vars"
    prometheus.io/port: "8080"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  - port: 10334
    targetPort: 10334
    name: locator
  - port: 3768
    targetPort: 3768
    name: zeppelin
  - port: 1531
    targetPort: 1531
    name: thrift
  - port: 1527
    targetPort: 1527
    name: jdbc
  clusterIP: None
  selector:
    app: snappydata-server
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: snappydata-leader
  labels:
    app: snappydata-leader
  annotations:
    # This is needed to make the peer-finder work properly and to help avoid
    # edge cases where instance 0 comes up after losing its data and needs to
    # decide whether it should create a new cluster or try to join an existing
    # one. If it creates a new cluster when it should have joined an existing
    # one, we'd end up with two separate clusters listening at the same service
    # endpoint, which would be very bad.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: "_status/vars"
    prometheus.io/port: "8080"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  - port: 10334
    targetPort: 10334
    name: locator
  - port: 3768
    targetPort: 3768
    name: zeppelin
  - port: 1531
    targetPort: 1531
    name: thrift
  - port: 1527
    targetPort: 1527
    name: jdbc
  clusterIP: None
  selector:
    app: snappydata-leader
---
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
  name: snappydata-locator
spec:
  serviceName: "snappydata-locator"
  replicas: 2
  template:
    metadata:
      labels:
        app: snappydata-locator
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      containers:
      - name: snappydata-locator
        # Runs the current snappydata release
        image: snappydatainc/snappydata
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            memory: "2048Mi"
            cpu: "200m"
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        - containerPort: 10334
          name: locator
        - containerPort: 1527
          name: jdbc
        - containerPort: 1531
          name: thrift
        - containerPort: 7075
          name: jmx
        livenessProbe:
          tcpSocket:
            port: 10334
          initialDelaySeconds: 80
        readinessProbe:
          tcpSocket:
            port: 10334
          initialDelaySeconds: 80
        volumeMounts:
        - name: datadir
          mountPath: /snappydata
        command:
          - "/bin/bash"
          - "-ecx"
          - |
            exec /bin/bash -c "mkdir -p /snappydata/locator && /opt/snappydata/bin/snappy-shell locator start -dir=/snappydata/locator -locators=snappydata-locator-public:10334 -peer-discovery-address=$(hostname -f) -membership-port-range=21000-21020 -client-bind-address=$(hostname -f)  -J-Duser.timezone=UTC -enable-network-partition-detection=true -jmx-manager=true -jmx-manager-http-port=7075 -jmx-manager-start=true && tail -f /snappydata/locator/snappylocator.log"
        lifecycle:
          preStop:
            exec:
              command:
              - /opt/snappydata/bin/snappy-shell locator stop -dir=/snappydata/locator
      terminationGracePeriodSeconds: 60
      volumes:
      - name: datadir
        persistentVolumeClaim:
          claimName: datadir
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 5Gi
---
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
  name: snappydata-server
spec:
  serviceName: "snappydata-server"
  replicas: 1
  template:
    metadata:
      labels:
        app: snappydata-server
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      containers:
      - name: snappydata-server
        # Runs the current snappydata release
        image: snappydatainc/snappydata
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            memory: "4096Mi"
            cpu: "1000m"
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        - containerPort: 10334
          name: locator
        - containerPort: 1527
          name: jdbc
        - containerPort: 1531
          name: thrift
        - containerPort: 21000
          name: membership-0000
        - containerPort: 21001
          name: membership-0001
        - containerPort: 21002
          name: membership-0002
        - containerPort: 21003
          name: membership-0003
        - containerPort: 21004
          name: membership-0004
        - containerPort: 21005
          name: membership-0005
        - containerPort: 21006
          name: membership-0006
        - containerPort: 21007
          name: membership-0007
        - containerPort: 21008
          name: membership-0008
        - containerPort: 21009
          name: membership-0009
        - containerPort: 21010
          name: membership-0010
        - containerPort: 21011
          name: membership-0011
        - containerPort: 21012
          name: membership-0012
        - containerPort: 21013
          name: membership-0013
        - containerPort: 21014
          name: membership-0014
        - containerPort: 21015
          name: membership-0015
        - containerPort: 21016
          name: membership-0016
        - containerPort: 21017
          name: membership-0017
        - containerPort: 21018
          name: membership-0018
        - containerPort: 21019
          name: membership-0019
        - containerPort: 21020
          name: membership-0020
        livenessProbe:
          tcpSocket:
            port: 1527
          initialDelaySeconds: 160
        readinessProbe:
          tcpSocket:
            port: 1527
          initialDelaySeconds: 160
        volumeMounts:
        - name: datadir
          mountPath: /snappydata
        command:
          - "/bin/bash"
          - "-ecx"
          - |
            exec /bin/bash -c "mkdir -p /snappydata/server && /opt/snappydata/bin/snappy-shell server start -dir=/snappydata/server -bind-address=$(hostname -f) -locators=snappydata-locator-public:10334 -dir=/snappydata/server -client-bind-address=$(hostname -f) -J-Duser.timezone=UTC -J-Djava.net.preferIPv4Stack=true -thrift-server-address=$(hostname -f) -heap-size=4096m -thrift-server-port=1531 -thrift-binary-protocol=true -membership-port-range=21000-21020 -enable-network-partition-detection=true  && tail -f /snappydata/server/snappyserver.log"
        lifecycle:
          preStop:
            exec:
              command:
              - /opt/snappydata/bin/snappy-shell server stop -dir=/snappydata/server
      terminationGracePeriodSeconds: 60
      volumes:
      - name: datadir
        persistentVolumeClaim:
          claimName: datadir
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 5Gi
---
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
  name: snappydata-leader
spec:
  serviceName: "snappydata-leader"
  replicas: 1
  template:
    metadata:
      labels:
        app: snappydata-leader
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
    spec:
      containers:
      - name: snappydata-leader
        # Runs the current snappydata release
        image: snappydatainc/snappydata
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            memory: "2048Mi"
            cpu: "1000m"
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        - containerPort: 10334
          name: locator
        - containerPort: 1527
          name: jdbc
        - containerPort: 21000
          name: membership-0000
        - containerPort: 21001
          name: membership-0001
        - containerPort: 21002
          name: membership-0002
        - containerPort: 21003
          name: membership-0003
        - containerPort: 21004
          name: membership-0004
        - containerPort: 21005
          name: membership-0005
        - containerPort: 21006
          name: membership-0006
        - containerPort: 21007
          name: membership-0007
        - containerPort: 21008
          name: membership-0008
        - containerPort: 21009
          name: membership-0009
        - containerPort: 21010
          name: membership-0010
        - containerPort: 21011
          name: membership-0011
        - containerPort: 21012
          name: membership-0012
        - containerPort: 21013
          name: membership-0013
        - containerPort: 21014
          name: membership-0014
        - containerPort: 21015
          name: membership-0015
        - containerPort: 21016
          name: membership-0016
        - containerPort: 21017
          name: membership-0017
        - containerPort: 21018
          name: membership-0018
        - containerPort: 21019
          name: membership-0019
        - containerPort: 21020
          name: membership-0020
        livenessProbe:
          httpGet:
            path: /
            port: 4040
          initialDelaySeconds: 160
        readinessProbe:
          httpGet:
            path: /
            port: 4040
          initialDelaySeconds: 160
        volumeMounts:
        - name: datadir
          mountPath: /snappydata
        command:
          - "/bin/bash"
          - "-ecx"
          - |
            exec /bin/bash -c "timeout 2 bash -c \"</dev/tcp/snappydata-server-public/1527\" && mkdir -p /snappydata/leader && /opt/snappydata/bin/snappy-shell leader start -dir=/snappydata/leader -locators=snappydata-locator-public:10334 -client-bind-address=$(hostname -f) -heap-size=4096m -J-Duser.timezone=UTC -membership-port-range=21000-21020 -J-XX:MaxPermSize=512m -enable-network-partition-detection=true && tail -f /snappydata/leader/snappyleader.log"
        lifecycle:  
          preStop: 
            exec:
              command:
              - /opt/snappydata/bin/snappy-shell leader stop -dir=/snappydata/leader
      terminationGracePeriodSeconds: 60
      volumes:
      - name: datadir
        persistentVolumeClaim:
          claimName: datadir
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 5Gi